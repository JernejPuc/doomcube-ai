default:
    trainer: ppo
    normalize: false
    vis_encode_type: simple

DoomCubeSolo:
    trainer: ppo

    # Trainer-agnostic
    summary_freq: 10000
    batch_size: 64
    buffer_size: 2048
    hidden_units: 256
    learning_rate: 3.0e-4
    learning_rate_schedule: linear
    max_steps: 1.0e+7
    num_layers: 2
    time_horizon: 512
    vis_encode_type: simple

    # PPO-specific
    beta: 0.005
    epsilon: 0.2
    lambd: 0.95
    num_epoch: 3

    # LSTM
    use_recurrent: true
    memory_size: 128
    sequence_length: 64

    # Pretraining through imitation
    behavioral_cloning:
        strength: 0.5
        steps: 150000
        demo_path: dcsolo.demo

    # Reward
    reward_signals:
        extrinsic:
            strength: 1.0
            gamma: 0.99
        curiosity:
            strength: 0.025
            gamma: 0.99
            encoding_size: 256
            learning_rate: 3.0e-4
        gail:
            strength: 0.05
            gamma: 0.99
            encoding_size: 128
            learning_rate: 3.0e-4
            use_actions: true
            demo_path: dcsolo.demo

DoomCubeAI:
    trainer: ppo

    # Trainer-agnostic
    summary_freq: 10000
    batch_size: 1024
    buffer_size: 10240
    hidden_units: 256
    learning_rate: 3.0e-4
    learning_rate_schedule: constant
    max_steps: 5.0e+7
    num_layers: 2
    time_horizon: 1000
    vis_encode_type: simple

    # PPO-specific
    beta: 0.005
    epsilon: 0.2
    lambd: 0.95
    num_epoch: 3

    # LSTM
    use_recurrent: true
    memory_size: 128
    sequence_length: 64

    # Reward
    reward_signals:
        extrinsic:
            strength: 1.0
            gamma: 0.99
    
    # Opponent configuration
    self_play:
        window: 10
        play_against_current_self_ratio: 0.5
        save_steps: 50000
        swap_steps: 50000
        team_change: 100000
